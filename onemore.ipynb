{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import json\n",
    "from collections import Counter\n",
    "import spacy\n",
    "from itertools import chain \n",
    "from collections import OrderedDict\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict\n",
    "from scipy.stats import entropy\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import pipeline\n",
    "unmasker = pipeline(\"fill-mask\", model=\"bert-base-uncased\")\n",
    "nltk.download('stopwords')\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "stopwordlist = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#processing functions \n",
    "def transform_dict(data):\n",
    "    if \"text\" in data:  \n",
    "        match = re.search(r'>>(\\d+)', data[\"text\"]) \n",
    "        if match:\n",
    "            data[\"replyto\"] = match.group(1)  \n",
    "            data[\"text\"] = re.sub(r'>>\\d+', '', data[\"text\"]).strip() \n",
    "    return data\n",
    "\n",
    "def convert_key_to_string(dict_list, key):\n",
    "    for d in dict_list:\n",
    "        if key in d and isinstance(d[key], int):\n",
    "            d[key] = str(d[key])  # Convert the value to a string\n",
    "    return dict_list\n",
    "\n",
    "# functions to get the threads with the word we are interested in \n",
    "def get_no_if_text_in_string(lst, string):\n",
    "    pattern = rf'\\b{re.escape(string)}\\b'\n",
    "    return [d['no'] for d in lst if 'text' in d and re.search(pattern, d['text'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the datasets \n",
    "## the full dictionary w/ all post data\n",
    "with open ('sets/cleanarchive.json', 'r') as file: \n",
    "    data = json.load(file)\n",
    "v = list(chain.from_iterable(data)) \n",
    "a = [transform_dict(x) for x in v]\n",
    "a = [x for x in a if isinstance(x, dict)]\n",
    "b = convert_key_to_string(a, 'no')\n",
    "\n",
    "## a list of the post's ID in thread order. each list represents a thread\n",
    "with open('nested_list.txt', 'r') as f:\n",
    "    me_loaded = json.load(f)\n",
    "me_flat = [item for sublist in me_loaded for item in sublist]\n",
    "me_moop = [[str(x) for x in l] for l in me_flat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_from_id(a_id_number):\n",
    "    the_text = [x['text'] for x in b if x['no'] == a_id_number]\n",
    "    return the_text\n",
    "\n",
    "def get_full_and_mentions(a_word):\n",
    "    lista = get_no_if_text_in_string(a, a_word)\n",
    "    list1 = [str(x) for x in lista]\n",
    "    def get_consecutive_lists(a_word):\n",
    "        return [\n",
    "            sublist for sublist in me_moop\n",
    "            if isinstance(sublist, list) and any(\n",
    "                list1[i] in sublist and list1[i + 1] in sublist\n",
    "                for i in range(len(list1) - 1)\n",
    "            )\n",
    "        ]\n",
    "    def get_word_ids(the_thread):\n",
    "        return [item for item in the_thread if item in list1]\n",
    "    \n",
    "    def filter_lists_by_last_id(data):\n",
    "        return list({lst[-1]: lst for lst in sorted(data, key=len, reverse=True) if lst}.values())  \n",
    "    conseclist = get_consecutive_lists(a_word)\n",
    "    filtered_conseclist = filter_lists_by_last_id(conseclist)\n",
    "    threads_and_id = list(map(get_word_ids, filtered_conseclist))\n",
    "    data = list(zip(filtered_conseclist, threads_and_id))\n",
    "    df = pd.DataFrame(data, columns=['Full Threads', 'Has Words'])\n",
    "    df['Word_Pos'] = df.apply(lambda x: [x['Full Threads'].index(m) for m in x['Has Words']], axis=1)\n",
    "    df['Convo_Length'] = df['Full Threads'].apply(len)\n",
    "    df['ID'] = df.index\n",
    "    df['sentences'] = df['Has Words'].apply(lambda x: [get_text_from_id(str(m)) for m in x])\n",
    "    return df\n",
    "\n",
    "def preprocess_sentences(sentences, target_word):\n",
    "    def normalize(text):\n",
    "        table = str.maketrans('', '', string.punctuation)\n",
    "        return text.translate(table).lower()\n",
    "    normalized_target = normalize(target_word)\n",
    "    masked_sentences = []\n",
    "    for sentence in sentences:\n",
    "        words = sentence.split()\n",
    "        regex = rf'\\b{re.escape(normalized_target)}\\b'\n",
    "        if re.search(regex, normalize(\" \".join(words))):\n",
    "            masked_sentence = \" \".join(\"[MASK]\" if re.fullmatch(regex, normalize(word)) else word for word in words)\n",
    "            masked_sentences.append(masked_sentence)\n",
    "    return masked_sentences\n",
    "\n",
    "def analyze_top1_and_similarity(masked_sentences, stopwords, max_length=512, top_k=10):\n",
    "    tokenizer = unmasker.tokenizer\n",
    "    top1_candidates = []\n",
    "    sentence_candidates = []\n",
    "    pos_similarity_scores = []\n",
    "    similarity_scores = []\n",
    "    \n",
    "    for sentence in masked_sentences:\n",
    "        if \"[MASK]\" not in sentence:\n",
    "            continue\n",
    "        tokenized = tokenizer.encode(sentence, add_special_tokens=True)\n",
    "        if len(tokenized) > max_length:\n",
    "            continue\n",
    "        predictions = unmasker(sentence)\n",
    "        if not (isinstance(predictions, list) and all(isinstance(pred, dict) for pred in predictions)):\n",
    "            continue\n",
    "        valid_candidates = [\n",
    "            pred for pred in predictions \n",
    "            if pred['token_str'].strip() not in string.punctuation and pred['token_str'].lower() not in stopwords\n",
    "        ]\n",
    "        filtered_candidates = {pred['token_str']: pred['score'] for pred in valid_candidates}\n",
    "        sentence_candidates.append(filtered_candidates)\n",
    "        \n",
    "        if filtered_candidates:\n",
    "            top1_word = max(filtered_candidates, key=filtered_candidates.get)\n",
    "            top1_candidates.append(f\"{top1_word}: {filtered_candidates[top1_word]}\")\n",
    "            \n",
    "            # Determine POS of the top1_word and the [MASK] position\n",
    "            doc = nlp(sentence.replace(\"[MASK]\", top1_word))\n",
    "            masked_index = [i for i, token in enumerate(doc) if token.text == top1_word]\n",
    "            if masked_index:\n",
    "                masked_token_pos = doc[masked_index[0]].pos_\n",
    "                pos_scores = [\n",
    "                    pred['score'] for pred in predictions\n",
    "                    if nlp(pred['token_str'])[0].pos_ == masked_token_pos\n",
    "                ]\n",
    "                pos_similarity_scores.append(max(pos_scores) if pos_scores else 0)\n",
    "        else:\n",
    "            top1_candidates.append(\"No valid prediction\")\n",
    "            pos_similarity_scores.append(0)\n",
    "    \n",
    "    for i in range(len(sentence_candidates)):\n",
    "        for j in range(i + 1, len(sentence_candidates)):\n",
    "            dist_i = sentence_candidates[i]\n",
    "            dist_j = sentence_candidates[j]\n",
    "            common_words = set(dist_i.keys()).intersection(set(dist_j.keys()))\n",
    "            if not common_words:\n",
    "                similarity_scores.append(0)\n",
    "                continue\n",
    "            score_diff = np.mean([abs(dist_i[word] - dist_j[word]) for word in common_words])\n",
    "            similarity_scores.append(1 - score_diff)\n",
    "    \n",
    "    avg_similarity = np.mean(similarity_scores) if similarity_scores else 0\n",
    "    avg_pos_similarity = np.mean(pos_similarity_scores) if pos_similarity_scores else 0\n",
    "    \n",
    "    return {\n",
    "        \"Top-1 Candidates\": top1_candidates,\n",
    "        \"Average Similarity\": avg_similarity,\n",
    "        \"Average POS Similarity\": avg_pos_similarity\n",
    "    }\n",
    "\n",
    "def word_to_df(the_word):\n",
    "    dataf = get_full_and_mentions(the_word)\n",
    "    dataf['maskedsent'] = dataf['sentences'].apply(lambda x: [preprocess_sentences(m, the_word) for m in x])\n",
    "    dataf['mask'] = dataf['maskedsent'].apply(lambda x: list(chain.from_iterable(x))) \n",
    "    dataf['Similarity'] = dataf['mask'].apply(lambda x: analyze_top1_and_similarity(x, stopwordlist))\n",
    "    dataf['Sim_Score'] = dataf['Similarity'].apply(lambda x: x['Average Similarity'])\n",
    "    dataf['POS_Sim_Score'] = dataf['Similarity'].apply(lambda x: x['Average POS Similarity'])\n",
    "    return dataf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fulldf = word_to_df(\"gay\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#view df \n",
    "fulldf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the overall conversation length distributions with conversations in which ...the word... appeared \n",
    "def plot_side_by_side(data):\n",
    "    fiona_counts = [len(x) for x in data['Full Threads']]\n",
    "    filtered_data = data[data['Convo_Length'] > 5]\n",
    "    filtered_fiona_counts = [len(x) for x in filtered_data['Full Threads']]\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(fiona_counts, bins=30, edgecolor='black', alpha=0.7)\n",
    "    plt.title('All Conversations')\n",
    "    plt.xlabel('Length')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.hist(filtered_fiona_counts, bins=30, edgecolor='black', alpha=0.7)\n",
    "    plt.title('Conversations (Convo_Length > 5)')\n",
    "    plt.xlabel('Length')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_side_by_side(fulldf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at where each word occurrence... occurred in the entire conversation thread\n",
    "def visualize_word_positions(dataframe):\n",
    "    dataframe = dataframe.sort_values(by='Convo_Length', ascending=True).reset_index(drop=True)\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    for idx, row in dataframe.iterrows():\n",
    "        word_positions = row['Word_Pos']\n",
    "        convo_length = row['Convo_Length'] + 1\n",
    "        if len(word_positions) > 1:\n",
    "            ax.plot(word_positions, [idx] * len(word_positions), color='lightblue', alpha=0.6, linewidth=1)\n",
    "        ax.plot([0, convo_length - 1], [idx, idx], color='orange', alpha=0.3, linewidth=0.5)\n",
    "        ax.scatter(word_positions, [idx] * len(word_positions), color='lightblue', s=8)\n",
    "    ax.set_xlim(0, dataframe['Convo_Length'].max() + 1)\n",
    "    ax.set_ylim(-1, len(dataframe))\n",
    "    ax.set_xlabel('Conversation Index')\n",
    "    ax.set_ylabel('Conversation ID (Ordered by Convo Length)')\n",
    "    ax.set_title('Word Positions Across Conversations')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_word_positions(fulldf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only look at conversations where ... the word... appeared consecutively (in direct conversation)\n",
    "def is_consecutive(numbers):\n",
    "    return all(b - a == 1 for a, b in zip(numbers, numbers[1:]))\n",
    "da = fulldf[fulldf['Word_Pos'].apply(is_consecutive)]\n",
    "visualize_word_positions(da)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#view da\n",
    "da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the distribution of similarity scores in consecutive appearances\n",
    "def plot_overall_sim_score(data):\n",
    "    filtered_data = [x for x in data['Sim_Score']]\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.hist(filtered_data, bins=10, edgecolor='black', alpha=0.7)\n",
    "    plt.title(\"distribution of word confidence\")\n",
    "    plt.xlabel(\"Value\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.show()\n",
    "\n",
    "plot_overall_sim_score(da)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the relationship between similarity score and the length of the conversation that followed the interaction \n",
    "da['Convo_After'] = da.apply(lambda row: row['Convo_Length'] - row['Word_Pos'][-1], axis=1)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(da['Sim_Score'], da['Convo_After'], alpha=0.7)\n",
    "plt.title('Scatter Plot of Sim_Score vs. Convo_After')\n",
    "plt.xlabel('Sim_Score')\n",
    "plt.ylabel('Convo_After')\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the distribution of POS similarity scores in consecutive appearances\n",
    "def plot_overall_possim_score(data):\n",
    "    filtered_data = [x for x in data['POS_Sim_Score']]\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.hist(filtered_data, bins=10, edgecolor='black', alpha=0.7)\n",
    "    plt.title(\"distribution of word confidence by POS\")\n",
    "    plt.xlabel(\"Value\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.show()\n",
    "\n",
    "plot_overall_possim_score(da)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the relationship between pos similarity score and the length of the conversation that followed the interaction \n",
    "da['Convo_After'] = da.apply(lambda row: row['Convo_Length'] - row['Word_Pos'][-1], axis=1)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(da['POS_Sim_Score'], da['Convo_After'], alpha=0.7)\n",
    "plt.title('Scatter Plot of POS_Sim_Score vs. Convo_After')\n",
    "plt.xlabel('POS_Sim_Score')\n",
    "plt.ylabel('Convo_After')\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
