[
  {
    "objectID": "onemorequarto.html",
    "href": "onemorequarto.html",
    "title": "test run – ‘are political conversations meaning-agnostic?’",
    "section": "",
    "text": "Notice that we’re using BERT (https://research.google/pubs/bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding/) here.\n\n\n\nYou can download and test different words here: https://github.com/emzou/testrun\n\nimport pandas as pd \nimport numpy as np\nimport json\nfrom collections import Counter\nimport spacy\nfrom itertools import chain \nfrom collections import OrderedDict\nimport re\nimport string\nimport nltk\nimport matplotlib.pyplot as plt\nfrom nltk.corpus import stopwords\nfrom collections import defaultdict\nfrom scipy.stats import entropy\nnltk.download('stopwords')\nnlp = spacy.load(\"en_core_web_sm\")\nstopwordlist = stopwords.words('english')\nfrom transformers import AutoTokenizer\nfrom transformers import pipeline\nunmasker = pipeline(\"fill-mask\", model=\"bert-base-uncased\")"
  },
  {
    "objectID": "onemorequarto.html#section",
    "href": "onemorequarto.html#section",
    "title": "test run – ‘are political conversations meaning-agnostic?’",
    "section": "3.1 ",
    "text": "3.1 \nWe have a database of conversation threads, which show the lifespan of every conversation (Every conversation begins with the Opening Post, and travels the replies all the way to the last reply (which didn’t receive any replies))"
  },
  {
    "objectID": "onemorequarto.html#section-1",
    "href": "onemorequarto.html#section-1",
    "title": "test run – ‘are political conversations meaning-agnostic?’",
    "section": "3.2 ",
    "text": "3.2 \nInput a word, we search for all the conversation threads that have the word show up. We filter the conversation threads to only include threads where the word appears at least twice."
  },
  {
    "objectID": "onemorequarto.html#section-2",
    "href": "onemorequarto.html#section-2",
    "title": "test run – ‘are political conversations meaning-agnostic?’",
    "section": "3.3 ",
    "text": "3.3 \nThe word is ‘masked’ (literally, by the string [MASK]). - If the sentence is: “I enjoy watching the TV show House.” and the word you’re interested in is “House”, the sentence now looks like “I enjoy watching the TV show [MASK]”"
  },
  {
    "objectID": "onemorequarto.html#section-3",
    "href": "onemorequarto.html#section-3",
    "title": "test run – ‘are political conversations meaning-agnostic?’",
    "section": "3.4 ",
    "text": "3.4 \nWe ask BERT to give its best guesses for what word ‘MASK’ is, returning a list of candidate words and its confidence level for each candidate word. - We also use spaCy to guess the part of speech of the ‘MASK’ word. - We get a ‘similarity score’ and a ‘part of speech similarity score’ on how close the model’s guesses of the word in each sentence is to each other. (Importantly, not whether the model was right or not, but whether its guesses were similar)"
  },
  {
    "objectID": "onemorequarto.html#section-4",
    "href": "onemorequarto.html#section-4",
    "title": "test run – ‘are political conversations meaning-agnostic?’",
    "section": "3.5 ",
    "text": "3.5 \nWe also collected the ‘position’ that each word appeared in the conversation thread based on its index. We also collect how long the conversation went on after the words show up on the thread.\n\nfulldf = word_to_df(\"woman\")\n\n\n#view df \nfulldf\n\n\n\n\n\n\n\n\nFull Threads\nHas Words\nWord_Pos\nConvo_Length\nID\nsentences\nmaskedsent\nmask\nSimilarity\nSim_Score\nPOS_Sim_Score\n\n\n\n\n0\n[493485835, 493485195, 493484848, 493482005]\n[493485835, 493485195]\n[0, 1]\n4\n0\n[[I agree I literally never said that, I sai...\n[[I agree I literally never said that, I said ...\n[I agree I literally never said that, I said t...\n{'Top-1 Candidates': ['people: 0.1100629866123...\n0.986069\n0.068819\n\n\n1\n[493465403, 493465268, 493464900, 493464762, 4...\n[493465403, 493465268, 493454834]\n[0, 1, 18]\n29\n1\n[[&gt;converted by a woman &gt;lets her better his q...\n[[&gt;converted by a [MASK] &gt;lets her better his ...\n[&gt;converted by a [MASK] &gt;lets her better his q...\n{'Top-1 Candidates': ['woman: 0.23987351357936...\n0.803247\n0.258764\n\n\n2\n[493540659, 493540293, 493540200, 493539816, 4...\n[493540659, 493540200]\n[0, 2]\n15\n2\n[[&gt;And the ease with which he divested himself...\n[[&gt;And the ease with which he divested himself...\n[&gt;And the ease with which he divested himself ...\n{'Top-1 Candidates': ['woman: 0.78796994686126...\n0.872913\n0.851513\n\n\n3\n[493498180, 493495860, 493495252, 493494412, 4...\n[493498180, 493491291]\n[0, 4]\n12\n3\n[[4/ _bk1.htm%23First &gt;Some say Sabaoth has th...\n[[], [&gt;Perhaps the cry of “Hyes Attes! Hyes At...\n[&gt;Perhaps the cry of “Hyes Attes! Hyes Attes!”...\n{'Top-1 Candidates': ['woman: 0.66045790910720...\n0.000000\n0.660458\n\n\n4\n[493419991, 493419756, 493419641, 493419421, 4...\n[493419991, 493419756]\n[0, 1]\n9\n4\n[[&gt;You're never getting your foreskin back &gt;Yo...\n[[&gt;You're never getting your foreskin back &gt;Yo...\n[&gt;You're never getting your foreskin back &gt;You...\n{'Top-1 Candidates': ['man: 0.2606106996536255...\n0.915907\n0.320440\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n161\n[493489091, 493488911]\n[493489091, 493488911]\n[0, 1]\n2\n161\n[[I was 28, and trying to figure out how to ge...\n[[], []]\n[]\n{'Top-1 Candidates': [], 'Average Similarity':...\n0.000000\n0.000000\n\n\n162\n[493643702, 493643557]\n[493643702, 493643557]\n[0, 1]\n2\n162\n[[&gt;I have never met a woman Ftfy], [I have nev...\n[[&gt;I have never met a [MASK] Ftfy], [I have ne...\n[&gt;I have never met a [MASK] Ftfy, I have never...\n{'Top-1 Candidates': ['good: 0.035287994891405...\n0.000000\n0.238734\n\n\n163\n[493601585, 493600013]\n[493601585, 493600013]\n[0, 1]\n2\n163\n[[the liberals will never get elected anytime ...\n[[the liberals will never get elected anytime ...\n[the liberals will never get elected anytime s...\n{'Top-1 Candidates': ['No valid prediction', '...\n0.000000\n0.090640\n\n\n164\n[493655950, 493655791]\n[493655950, 493655791]\n[0, 1]\n2\n164\n[[I already have a woman I've never had a prob...\n[[I already have a [MASK] I've never had a pro...\n[I already have a [MASK] I've never had a prob...\n{'Top-1 Candidates': ['feeling: 0.471853792667...\n0.000000\n0.048756\n\n\n165\n[493411641, 493411534]\n[493411641, 493411534]\n[0, 1]\n2\n165\n[[&gt;woman is stupid, more news at 10], [Here's ...\n[[[MASK] is stupid, more news at 10], [Here's ...\n[[MASK] is stupid, more news at 10, Here's you...\n{'Top-1 Candidates': ['news: 0.053776908665895...\n0.000000\n0.037695\n\n\n\n\n166 rows × 11 columns\n\n\n\n\n# look at the overall conversation length distributions with conversations in which ...the word... appeared \ndef plot_side_by_side(data):\n    fiona_counts = [len(x) for x in data['Full Threads']]\n    filtered_data = data[data['Convo_Length'] &gt; 5]\n    filtered_fiona_counts = [len(x) for x in filtered_data['Full Threads']]\n    plt.figure(figsize=(14, 6))\n    plt.subplot(1, 2, 1)\n    plt.hist(fiona_counts, bins=30, edgecolor='black', alpha=0.7)\n    plt.title('All Conversations')\n    plt.xlabel('Length')\n    plt.ylabel('Frequency')\n    plt.subplot(1, 2, 2)\n    plt.hist(filtered_fiona_counts, bins=30, edgecolor='black', alpha=0.7)\n    plt.title('Conversations (Convo_Length &gt; 5)')\n    plt.xlabel('Length')\n    plt.ylabel('Frequency')\n    plt.tight_layout()\n    plt.show()\n\nplot_side_by_side(fulldf)\n\n\n\n\n\n\n\n\n\n# look at where each word occurrence... occurred in the entire conversation thread\ndef visualize_word_positions(dataframe):\n    dataframe = dataframe.sort_values(by='Convo_Length', ascending=True).reset_index(drop=True)\n    fig, ax = plt.subplots(figsize=(12, 6))\n    for idx, row in dataframe.iterrows():\n        word_positions = row['Word_Pos']\n        convo_length = row['Convo_Length'] + 1\n        if len(word_positions) &gt; 1:\n            ax.plot(word_positions, [idx] * len(word_positions), color='lightblue', alpha=0.6, linewidth=1)\n        ax.plot([0, convo_length - 1], [idx, idx], color='orange', alpha=0.3, linewidth=0.5)\n        ax.scatter(word_positions, [idx] * len(word_positions), color='lightblue', s=8)\n    ax.set_xlim(0, dataframe['Convo_Length'].max() + 1)\n    ax.set_ylim(-1, len(dataframe))\n    ax.set_xlabel('Conversation Index')\n    ax.set_ylabel('Conversation ID (Ordered by Convo Length)')\n    ax.set_title('Word Positions Across Conversations')\n    plt.tight_layout()\n    plt.show()\n\nvisualize_word_positions(fulldf)\n\n\n\n\n\n\n\n\n\n# only look at conversations where ... the word... appeared consecutively (in direct conversation)\ndef is_consecutive(numbers):\n    return all(b - a == 1 for a, b in zip(numbers, numbers[1:]))\nda = fulldf[fulldf['Word_Pos'].apply(is_consecutive)]\nvisualize_word_positions(da)\n\n\n\n\n\n\n\n\n\n#view da\nda\n\n\n\n\n\n\n\n\nFull Threads\nHas Words\nWord_Pos\nConvo_Length\nID\nsentences\nmaskedsent\nmask\nSimilarity\nSim_Score\nPOS_Sim_Score\n\n\n\n\n0\n[493485835, 493485195, 493484848, 493482005]\n[493485835, 493485195]\n[0, 1]\n4\n0\n[[I agree I literally never said that, I sai...\n[[I agree I literally never said that, I said ...\n[I agree I literally never said that, I said t...\n{'Top-1 Candidates': ['people: 0.1100629866123...\n0.986069\n0.068819\n\n\n4\n[493419991, 493419756, 493419641, 493419421, 4...\n[493419991, 493419756]\n[0, 1]\n9\n4\n[[&gt;You're never getting your foreskin back &gt;Yo...\n[[&gt;You're never getting your foreskin back &gt;Yo...\n[&gt;You're never getting your foreskin back &gt;You...\n{'Top-1 Candidates': ['man: 0.2606106996536255...\n0.915907\n0.320440\n\n\n6\n[493426163, 493425843, 493425346, 493425216, 4...\n[493426163, 493425843]\n[0, 1]\n18\n6\n[[it does, to lay with a man as you would lay ...\n[[it does, to lay with a man as you would lay ...\n[it does, to lay with a man as you would lay w...\n{'Top-1 Candidates': ['man: 0.2481773197650909...\n0.000000\n0.248177\n\n\n7\n[493644707, 493641149, 493640771, 493628486, 4...\n[493644707, 493641149]\n[0, 1]\n5\n7\n[[&gt;You're not entitled to sex with a woman at ...\n[[&gt;You're not entitled to sex with a [MASK] at...\n[&gt;You're not entitled to sex with a [MASK] at ...\n{'Top-1 Candidates': ['man: 0.5526390671730042...\n0.928478\n0.459630\n\n\n8\n[493402288, 493399994, 493398514, 493389986, 4...\n[493402288, 493399994]\n[0, 1]\n5\n8\n[[The first example is adultery. Its a cut off...\n[[The first example is adultery. Its a cut off...\n[The first example is adultery. Its a cut off ...\n{'Top-1 Candidates': ['woman: 0.89872688055038...\n0.980488\n0.921860\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n161\n[493489091, 493488911]\n[493489091, 493488911]\n[0, 1]\n2\n161\n[[I was 28, and trying to figure out how to ge...\n[[], []]\n[]\n{'Top-1 Candidates': [], 'Average Similarity':...\n0.000000\n0.000000\n\n\n162\n[493643702, 493643557]\n[493643702, 493643557]\n[0, 1]\n2\n162\n[[&gt;I have never met a woman Ftfy], [I have nev...\n[[&gt;I have never met a [MASK] Ftfy], [I have ne...\n[&gt;I have never met a [MASK] Ftfy, I have never...\n{'Top-1 Candidates': ['good: 0.035287994891405...\n0.000000\n0.238734\n\n\n163\n[493601585, 493600013]\n[493601585, 493600013]\n[0, 1]\n2\n163\n[[the liberals will never get elected anytime ...\n[[the liberals will never get elected anytime ...\n[the liberals will never get elected anytime s...\n{'Top-1 Candidates': ['No valid prediction', '...\n0.000000\n0.090640\n\n\n164\n[493655950, 493655791]\n[493655950, 493655791]\n[0, 1]\n2\n164\n[[I already have a woman I've never had a prob...\n[[I already have a [MASK] I've never had a pro...\n[I already have a [MASK] I've never had a prob...\n{'Top-1 Candidates': ['feeling: 0.471853792667...\n0.000000\n0.048756\n\n\n165\n[493411641, 493411534]\n[493411641, 493411534]\n[0, 1]\n2\n165\n[[&gt;woman is stupid, more news at 10], [Here's ...\n[[[MASK] is stupid, more news at 10], [Here's ...\n[[MASK] is stupid, more news at 10, Here's you...\n{'Top-1 Candidates': ['news: 0.053776908665895...\n0.000000\n0.037695\n\n\n\n\n116 rows × 11 columns\n\n\n\n\n# look at the distribution of similarity scores in consecutive appearances\ndef plot_overall_sim_score(data):\n    filtered_data = [x for x in data['Sim_Score']]\n    plt.figure(figsize=(8, 6))\n    plt.hist(filtered_data, bins=10, edgecolor='black', alpha=0.7)\n    plt.title(\"distribution of word confidence\")\n    plt.xlabel(\"Value\")\n    plt.ylabel(\"Frequency\")\n    plt.grid(axis='y', linestyle='--', alpha=0.7)\n    plt.show()\n\nplot_overall_sim_score(da)\n\n\n\n\n\n\n\n\n\n# look at the relationship between similarity score and the length of the conversation that followed the interaction \nda['Convo_After'] = da.apply(lambda row: row['Convo_Length'] - row['Word_Pos'][-1], axis=1)\nplt.figure(figsize=(8, 6))\nplt.scatter(da['Sim_Score'], da['Convo_After'], alpha=0.7)\nplt.title('Scatter Plot of Sim_Score vs. Convo_After')\nplt.xlabel('Sim_Score')\nplt.ylabel('Convo_After')\nplt.grid(True, linestyle='--', alpha=0.5)\nplt.show()\n\n\n\n\n\n\n\n\n\n# look at the distribution of POS similarity scores in consecutive appearances\ndef plot_overall_possim_score(data):\n    filtered_data = [x for x in data['POS_Sim_Score']]\n    plt.figure(figsize=(8, 6))\n    plt.hist(filtered_data, bins=10, edgecolor='black', alpha=0.7)\n    plt.title(\"distribution of word confidence by POS\")\n    plt.xlabel(\"Value\")\n    plt.ylabel(\"Frequency\")\n    plt.grid(axis='y', linestyle='--', alpha=0.7)\n    plt.show()\n\nplot_overall_possim_score(da)\n\n\n\n\n\n\n\n\n\n# look at the relationship between pos similarity score and the length of the conversation that followed the interaction \nda['Convo_After'] = da.apply(lambda row: row['Convo_Length'] - row['Word_Pos'][-1], axis=1)\nplt.figure(figsize=(8, 6))\nplt.scatter(da['POS_Sim_Score'], da['Convo_After'], alpha=0.7)\nplt.title('Scatter Plot of POS_Sim_Score vs. Convo_After')\nplt.xlabel('POS_Sim_Score')\nplt.ylabel('Convo_After')\nplt.grid(True, linestyle='--', alpha=0.5)\nplt.show()"
  }
]